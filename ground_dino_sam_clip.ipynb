{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import sys, os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "sys.path.append(\"./GroundingDINO/\")\n",
    "sys.path.append(\"segment-anything\")\n",
    "from GroundingDINO.groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import clip\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "from torchvision.ops import box_convert, box_iou\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"GroundingDINO/weights/inpaint_demo.jpg\"\n",
    "TEXT_PROMPT_CAPTURE = \"object on the hand\"\n",
    "TEXT_PROMPT_RECOGNIZE = \"object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float) -> torch.Tensor:\n",
    "    order = torch.argsort(-scores)\n",
    "    indices = torch.arange(bboxes.shape[0])\n",
    "    keep = torch.ones_like(indices, dtype=torch.bool)\n",
    "    for i in indices:\n",
    "        if keep[i]:\n",
    "            bbox = bboxes[order[i]]\n",
    "            iou = box_iou(bbox[None,...],(bboxes[order[i + 1:]]) * keep[i + 1:][...,None])\n",
    "            overlapped = torch.nonzero(iou > iou_threshold)\n",
    "            keep[overlapped + i + 1] = 0\n",
    "    return order[keep]\n",
    "\n",
    "def getJetColorRGB(v, vmin, vmax):\n",
    "    c = np.zeros((3))\n",
    "    if (v < vmin):\n",
    "        v = vmin\n",
    "    if (v > vmax):\n",
    "        v = vmax\n",
    "    dv = vmax - vmin\n",
    "    if (v < (vmin + 0.125 * dv)): \n",
    "        c[0] = 256 * (0.5 + (v * 4)) #B: 0.5 ~ 1\n",
    "    elif (v < (vmin + 0.375 * dv)):\n",
    "        c[0] = 255\n",
    "        c[1] = 256 * (v - 0.125) * 4 #G: 0 ~ 1\n",
    "    elif (v < (vmin + 0.625 * dv)):\n",
    "        c[0] = 256 * (-4 * v + 2.5)  #B: 1 ~ 0\n",
    "        c[1] = 255\n",
    "        c[2] = 256 * (4 * (v - 0.375)) #R: 0 ~ 1\n",
    "    elif (v < (vmin + 0.875 * dv)):\n",
    "        c[1] = 256 * (-4 * v + 3.5)  #G: 1 ~ 0\n",
    "        c[2] = 255\n",
    "    else:\n",
    "        c[2] = 256 * (-4 * v + 4.5) #R: 1 ~ 0.5                      \n",
    "    return c\n",
    "\n",
    "def ground_dino_predict(model, img_path, text_prompt, box_threshold=0.35, text_threshold=0.25, topK=10):\n",
    "    image_source, image = load_image(img_path)\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    # nms\n",
    "    keep = nms(boxes, logits, 0.5)\n",
    "    boxes = boxes[keep]\n",
    "    logits = logits[keep]\n",
    "\n",
    "    print(\"Predicted boxes:\", boxes.shape[0])\n",
    "\n",
    "    return boxes, logits\n",
    "\n",
    "def sam_predict(predictor, img_path, boxes):\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = image.copy()\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # get from object detector\n",
    "    h, w, _ = image.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h], device=boxes.device)\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
    "\n",
    "    input_boxes = xyxy.to(predictor.device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
    "\n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    return masks, xyxy, img_rgb\n",
    "\n",
    "def ground_dino_sam_predict(model, predictor, img_path, text_prompt, box_threshold=0.35, text_threshold=0.25):\n",
    "    boxes, logits = ground_dino_predict(model, img_path, text_prompt, box_threshold, text_threshold)\n",
    "    masks, boxes, img_rgb = sam_predict(predictor, img_path, boxes)\n",
    "    return masks, boxes, img_rgb\n",
    "\n",
    "def load_model_and_predict():\n",
    "    #Use GroundingDino to detect items.\n",
    "    print(\"Loading GroundingDINO model...\")\n",
    "    model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
    "\n",
    "    # Use SAM to generate the mask.\n",
    "    print(\"Loading Segment Anything model...\")\n",
    "    sam_checkpoint = \"segment-anything/sam_vit_h_4b8939.pth\"\n",
    "    model_type = \"vit_h\"\n",
    "    device = \"cuda\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=device) \n",
    "    predictor = SamPredictor(sam)\n",
    "\n",
    "    print(\"Loading clip ViT-B/32 model...\")\n",
    "    extractor, _ = clip.load(\"ViT-B/32\", device, jit=False)\n",
    "\n",
    "    return model, predictor, extractor\n",
    "\n",
    "def extract_saved_obj_features(model, predictor, extractor):\n",
    "    image_path = \"./reg\"\n",
    "    output_path = \"./extract_results\"\n",
    "    if not osp.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    else:\n",
    "        # remove all files\n",
    "        shutil.rmtree(output_path)\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    obj_list = [oi for oi in os.listdir(image_path) if not \"DS_Store\" in oi]\n",
    "    obj_list.sort()\n",
    "    \n",
    "    obj_features = []\n",
    "\n",
    "    for obj_dir in tqdm(obj_list):\n",
    "        obj_path = osp.join(image_path, obj_dir)\n",
    "        features_all = []\n",
    "        if not osp.exists(osp.join(output_path, obj_dir)):\n",
    "            os.mkdir(osp.join(output_path, obj_dir))\n",
    "        for obj in [oi for oi in os.listdir(obj_path) if not \"DS_Store\" in oi]:\n",
    "            img_path = osp.join(obj_path, obj)\n",
    "\n",
    "            # ground dino and sam inference\n",
    "            masks, boxes, img_rgb = ground_dino_sam_predict(model, predictor, img_path, TEXT_PROMPT_CAPTURE)\n",
    "            x1, y1, x2, y2 = boxes[0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            \n",
    "            # mask\n",
    "            masked_img = img_rgb * masks[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            cv2.imwrite(osp.join(output_path, obj_dir, \"mask_\"+obj), cv2.cvtColor(masked_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # crop and pad to center\n",
    "            masked_img = masked_img[y1:y2, x1:x2, :]\n",
    "            h, w, _ = masked_img.shape\n",
    "            size = max(h, w)\n",
    "            img_pad = np.zeros((size, size, 3)).astype(np.uint8)\n",
    "            img_pad[size//2-h//2:size//2+(h-h//2), size//2-w//2:size//2+(w-w//2)] = masked_img\n",
    "            img_pad = cv2.resize(img_pad, (224, 224))\n",
    "            cv2.imwrite(osp.join(output_path, obj_dir, \"mask_crop_\"+obj), cv2.cvtColor(img_pad, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # normalize\n",
    "            image = img_pad.astype(np.float32) / 255.\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).cuda()\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(image.device)\n",
    "            std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(image.device)\n",
    "            mean = mean.view(1, -1, 1, 1)\n",
    "            std = std.view(1, -1, 1, 1)\n",
    "\n",
    "            image = (image - mean) / std\n",
    "\n",
    "            features = extractor.encode_image(image)\n",
    "            features_all.append(features)\n",
    "\n",
    "        features_all = torch.cat(features_all, dim=0)\n",
    "        obj_features.append(features_all.unsqueeze(0))\n",
    "\n",
    "    obj_features = torch.cat(obj_features, dim=0)\n",
    "    obj_features = {\n",
    "        \"features\": obj_features,\n",
    "        \"obj_list\": obj_list\n",
    "    }\n",
    "    torch.save(obj_features, osp.join(output_path, \"obj_features.pt\"))\n",
    "    print(\"saved obj_features.pt!!\")\n",
    "\n",
    "    return obj_features\n",
    "\n",
    "def recognize_pipeline(model, predictor, extractor, obj_features, recognize_img_path, box_threshold=0.35, text_threshold=0.25, idx=0):\n",
    "\n",
    "    obj_list = obj_features[\"obj_list\"]\n",
    "    obj_features = obj_features[\"features\"]\n",
    "    \n",
    "    if not osp.exists(f\"results/{idx}\"):\n",
    "        os.mkdir(f\"results/{idx}\")\n",
    "    else:\n",
    "        # remove all files\n",
    "        shutil.rmtree(f\"results/{idx}\")\n",
    "        os.mkdir(f\"results/{idx}\")\n",
    "\n",
    "    # ground dino and sam inference\n",
    "    masks, boxes, img_rgb = ground_dino_sam_predict(model, predictor, recognize_img_path, TEXT_PROMPT_RECOGNIZE, box_threshold, text_threshold)\n",
    "    img_rgb_copy = img_rgb.copy()\n",
    "\n",
    "    for obj_idx, (boxi, maski) in enumerate(zip(boxes, masks)):\n",
    "        x1, y1, x2, y2 = boxi.cpu().numpy()\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        area = (x2-x1) * (y2-y1)\n",
    "        if area > 1000*1000:\n",
    "            continue\n",
    "\n",
    "        # mask\n",
    "        masked_img = img_rgb * maski.cpu().numpy().transpose(1, 2, 0)\n",
    "        cv2.imwrite(f\"./results/{idx}/mask_{obj_idx}.jpg\", cv2.cvtColor(masked_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # crop and pad to center\n",
    "        masked_img = masked_img[y1:y2, x1:x2, :]\n",
    "        h, w, _ = masked_img.shape\n",
    "        size = max(h, w)\n",
    "        img_pad = np.zeros((size, size, 3)).astype(np.uint8)\n",
    "        img_pad[size//2-h//2:size//2+(h-h//2), size//2-w//2:size//2+(w-w//2)] = masked_img\n",
    "        img_pad = cv2.resize(img_pad, (224, 224))\n",
    "        cv2.imwrite(f\"./results/{idx}/mask_crop_{obj_idx}.jpg\", cv2.cvtColor(img_pad, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # normalize\n",
    "        image = img_pad.astype(np.float32) / 255.\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).cuda()\n",
    "        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(image.device)\n",
    "        std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(image.device)\n",
    "        mean = mean.view(1, -1, 1, 1)\n",
    "        std = std.view(1, -1, 1, 1)\n",
    "\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        # extract features\n",
    "        features = extractor.encode_image(image)  # (1, 512)\n",
    "\n",
    "        similarity = cosine_similarity(features, obj_features, dim=-1)\n",
    "        sim_order = torch.argmax(similarity, dim=-1)\n",
    "\n",
    "        res_text = []\n",
    "        max_score_idx = 0\n",
    "        max_score = 0\n",
    "        for i in range(len(sim_order)):\n",
    "            sim_t = similarity[i][sim_order[i]]\n",
    "            if sim_t > max_score:\n",
    "                max_score = sim_t\n",
    "                max_score_idx = i\n",
    "            res_text.append(f\"{obj_list[i]}: {sim_t:.3f}\")\n",
    "\n",
    "        # write to image\n",
    "        y1_t = y1\n",
    "        for rti_idx, rti in enumerate(res_text):\n",
    "            if not rti_idx == max_score_idx:\n",
    "                cv2.putText(img_rgb_copy, rti, (x1+6, y1_t+40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(img_rgb_copy, rti, (x1+6, y1_t+40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "            y1_t += 40\n",
    "        cv2.rectangle(img_rgb_copy, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imwrite(f\"./results/{idx}/res_{box_threshold}_{text_threshold}.jpg\", cv2.cvtColor(img_rgb_copy, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GroundingDINO model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanai-lab/chen-j/.conda/envs/L20/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Segment Anything model...\n",
      "Loading clip ViT-B/32 model...\n"
     ]
    }
   ],
   "source": [
    "model, predictor, extractor = load_model_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_features = extract_saved_obj_features(model, predictor, extractor)\n",
    "obj_features = torch.load(\"./extract_results/obj_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted boxes: 13\n"
     ]
    }
   ],
   "source": [
    "for i in [13]:\n",
    "    recognize_pipeline(model, predictor, extractor, obj_features, f\"./test_img/reg_all_{i}.jpg\", 0.1, 0.15, idx=i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
